# Gradient Descent: The Batch Update
# Градиентный спуск: пакетное обновление

Примечание. Для пакетного градиентного спуска важно сделать переменные совместимыми 
для умножения и скалярного произведения путем транспонирования (при необходимости). 
Размеры входных параметров указаны в скобках под объяснением функций для лучшего понимания концепций.

### функция `train`:
* она принимает данные `X`, метки `Y`, веса `weights`, смещение `bias` и скорость обучения `learning_rate`.
* цикл `for` перебирает `epchs` раз, обновляя веса и смещения в периодическом режиме. В эпоху:
  - вызываем функцию `forward propagation` для вычисления прогнозируемого значения и сохраняем возвращаемое 
  значение в `Y_predicted`.
  - Вызываем функцию `compute_error` для вычисления ошибки в каждую эпоху и сохраняем возвращаемое значение 
  в массиве `losses`.
  - Вызываем функцию `gradient` для вычисления градиента ошибки относительно весов и смещения.
  - Вызываем функцию `update_parameters`, которая возвращает обновленное значение весов и смещения.
  
### функция `forward_propagation`:
Функция принимает входные данные `X`, вес `W` и смещение `bias`.
* Размеры `X` 10 * 2 и `W` 2 * 1.
* `np.dot` вычисляет `weighted_sum` размер 10 * 1:

      weighted_sum(10 * 1) = X(10 * 2) . W(2 * 1) + bias (1 * 1) 
  
* Функция активации `sigmoid` применяется к `weighted_sum`.
* Получаем выходной вектор размером 10 * 1.

### функция `gradient_descent`:
Функция принимает входные данные `X`, фактические метки `Y_predicted` и целевые метки `Y`.
* Ошибка вычисляется путем вычитания `Y` из `Y_predicted`. Обратите внимание, что 
размеры обоих составляют 10 * 1, и мы получаем на выходе 10 * 1.

      Error(10 * 1) = Y_predicted(10 * 1) - Y(10 * 1)
      
* Затем вычисляем производную ошибки по весу. Входной вектор `X` имеет размер 10 * 2. Мы должны использовать 
транспонирование, чтобы сделать его совместимым со скалярным произведением с `Error`.

Примечание. При вычислении производной ошибки по смещению мы суммируем значения по оси Y, чтобы получить 
вектор размером 1 * 1.

      dW(2 * 1) = X.T(2 * 4) . Error(4 * 1) 
      
* Производной ошибки по смещению является ошибка:

      db(1 * 1) = sum(Error(10 * 1))
      
### функция `update_parameters`:
Функция принимает параметры, веса Wи смещение bвместе с их производными dWи, dbсоответственно, 
применяет правило обновления:

      W(2 * 1) = W(2 * 1) -  learning_rate (1 * 1) * dW(2 * 1)
      
      b(1 * 1) = b(1 * 1) -  learning_rate  * db(1 * 1)
      
Этот метод обновления весов известен как **пакетный градиентный спуск**.

